{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20cbad63-dc10-41b1-aa6c-a1a80828d538",
   "metadata": {},
   "source": [
    "## Q1. What is Min-Max scaling, and how is it used in data preprocessing? Provide an example to illustrate its application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f921276a-9195-453e-9a39-382ef901e209",
   "metadata": {},
   "source": [
    "## Ans:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae82433-93af-489a-a6cc-ad250b165eae",
   "metadata": {},
   "source": [
    "Min-Max scaling, also known as Min-Max normalization, is a data preprocessing technique used to rescale numeric features in a dataset to a specific range, typically between 0 and 1. It is a linear transformation that preserves the relative differences between data points while ensuring that the data falls within a specified interval.\n",
    "\n",
    "The formula for Min-Max scaling is as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "039ef2cc-7f6c-4431-85d8-49deafedc2f2",
   "metadata": {},
   "source": [
    "$X_{norm} = \\frac{X-X_{min}}{X_{max}-X_{min}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "021fa558-cebf-4008-bfac-c9b21a36e42b",
   "metadata": {},
   "source": [
    "$X_{norm}$ : is the scaled value of the original feature X.\\\n",
    "$X_{min}$ : is the minimum value in the original feature X.\\\n",
    "$X_{max}$ : is the maximum value in the original feature X."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b05d55c-df12-4604-a159-4c7d50764dae",
   "metadata": {},
   "source": [
    "Here's an example to illustrate how Min-Max scaling works:\n",
    "\n",
    "Suppose you have a dataset of ages, and you want to scale them using Min-Max scaling to bring them into the range [0, 1]. Your dataset might look like this:\n",
    "Age = [25, 30, 20, 35, 40]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e5e25a4-a87d-42f4-a60a-566d73abe0cc",
   "metadata": {},
   "source": [
    "1. Find the minimum and maximum values of the \"Age\" feature:\\\n",
    "        Minimum (X_min) = 20\\\n",
    "        Maximum (X_max) = 40\n",
    "\n",
    "2. Apply the Min-Max scaling formula to each age in the dataset:\n",
    "        For the first age (25):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ba041e-84c3-4492-958b-55e726ac2ef4",
   "metadata": {},
   "source": [
    "$X_{norm}=\\frac{25−20}{40−20}=\\frac{5}{20}=0.25$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd054cb3-afd9-47a3-b762-0db84cf2cc56",
   "metadata": {},
   "source": [
    "After applying Min-Max scaling, the scaled dataset will look like this:\n",
    "Age=[0.25, 0.5, 0.0, 0.75,1.0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a93be8a8-cf9b-40f3-87d4-3b91544030f8",
   "metadata": {},
   "source": [
    "## Q2. What is the Unit Vector technique in feature scaling, and how does it differ from Min-Max scaling? Provide an example to illustrate its application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66db5b95-cca6-4f63-b7da-4a68ed1dafe7",
   "metadata": {},
   "source": [
    "## Ans:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53346a07-5cf7-4503-b539-41aa1f574fc9",
   "metadata": {},
   "source": [
    "The Unit Vector technique in feature scaling, often referred to as \"Normalization,\" is a method used to scale features in a dataset in such a way that each feature vector (data point) has a Euclidean norm (vector length) of 1. This technique is particularly useful when you want to ensure that the direction of the data points remains the same while standardizing their magnitudes.\n",
    "\n",
    "The formula for normalizing a feature vector using the Unit Vector technique is as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe8ae64-b9c1-4cbe-8835-52204d30521d",
   "metadata": {},
   "source": [
    "$X_{norm}=\\frac{X}{||X||}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2befabf5-44d7-47b4-86f9-dd7fbc5af2b6",
   "metadata": {},
   "source": [
    "Where:\n",
    "\n",
    "$X_{norm}$ : is the normalized feature vector.\\\n",
    "X : is the original feature vector.\\\n",
    "∥X∥ is the Euclidean norm (L2 norm) of the feature vector X, calculated as the square root of the sum of the squares of its elements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5343fbb0-037d-49e6-8cd0-90e3c5fb839e",
   "metadata": {},
   "source": [
    "Here's an example to illustrate the Unit Vector technique:\n",
    "\n",
    "Suppose you have a dataset of 2D vectors (features), and you want to normalize these vectors:\\\n",
    "Feature = [[3, 4], [1, 2], [6, 8]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b6674ba-057c-402f-8e7b-46496ca7c361",
   "metadata": {},
   "source": [
    "To normalize these vectors using the Unit Vector technique, follow these steps:\n",
    "1. Calculate the Euclidean norm (∥X∥) for each vector:\\\n",
    "For the first vector [3, 4]:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "543d3294-dc69-44b1-8278-4a11f4f16cb0",
   "metadata": {},
   "source": [
    "∥X∥ = $\\sqrt{3^{2}+4^{2}}=\\sqrt{9+16}=5$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "528f13ad-3524-4546-a7b3-4ce7bc2700cf",
   "metadata": {},
   "source": [
    "2. Normalize each vector by dividing it by its Euclidean norm:\\\n",
    "For the first vector [3, 4]:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f7b811-2949-4522-b40d-9e13e9efacd2",
   "metadata": {},
   "source": [
    "$X_{norm}=\\frac{[3,4]}{5}=[3/5,4/5]$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff86ac18-b87c-48ff-b9bd-0a6be0b7b355",
   "metadata": {},
   "source": [
    "Repeat this process for each vector. \n",
    "\n",
    "After applying the Unit Vector technique, the normalized dataset will look like this:\\\n",
    "Normalized_Feature=[[0.6,0.8], [0.447,0.894], [0.6,0.8]]\n",
    "\n",
    "Notice that the direction of each vector is preserved, but their magnitudes are scaled to have a length of 1. This normalization is useful in various machine learning algorithms, such as clustering algorithms like K-Means, where the distance between data points is important. Normalization ensures that features with different scales do not dominate the distance calculations.\n",
    "\n",
    "In contrast to Min-Max scaling, which scales features to a specific range (e.g., [0, 1]), Unit Vector normalization focuses on the direction of the vectors and doesn't constrain them to a specific range. Therefore, the choice between Min-Max scaling and Unit Vector normalization depends on the specific requirements of your problem and the algorithms you plan to use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "116ea0a5-f1b5-47f6-bc78-a15246deedaa",
   "metadata": {},
   "source": [
    "## Q3. What is PCA (Principle Component Analysis), and how is it used in dimensionality reduction? Provide an example to illustrate its application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ffdf4bd-f423-4b95-81e6-3de43e9dfd6c",
   "metadata": {},
   "source": [
    "## Ans:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05935f01-e5b6-45a8-8fb5-260cabe0ec28",
   "metadata": {},
   "source": [
    "PCA, or Principal Component Analysis, is a widely used technique in the field of machine learning and data analysis for dimensionality reduction and feature extraction. It helps in simplifying complex datasets by transforming them into a lower-dimensional representation while retaining as much of the relevant information as possible. PCA achieves this by identifying the principal components or directions of maximum variance in the data and projecting the data onto these components.\n",
    "\n",
    "Here's a step-by-step explanation of how PCA works:\n",
    "\n",
    "    Data Standardization: If your dataset contains features with different scales, it's often a good practice to standardize them (e.g., using Z-score normalization) to give all features equal importance in the PCA process.\n",
    "\n",
    "    Covariance Matrix Calculation: PCA begins by calculating the covariance matrix of the standardized data. The covariance matrix provides information about the relationships and variances between pairs of features.\n",
    "\n",
    "    Eigenvalue and Eigenvector Computation: The next step is to calculate the eigenvalues and eigenvectors of the covariance matrix. Eigenvalues represent the amount of variance explained by each corresponding eigenvector (principal component).\n",
    "\n",
    "    Sorting Eigenvalues: Eigenvalues are typically sorted in descending order. This ordering helps identify the most significant principal components, as the highest eigenvalues correspond to the directions of maximum variance in the data.\n",
    "\n",
    "    Selecting Principal Components: To reduce the dimensionality of the dataset, you can choose a subset of the top k eigenvectors/principal components. These top components capture the most variance in the data and are used to project the data onto a lower-dimensional subspace.\n",
    "\n",
    "    Projection: The final step involves projecting the original data onto the selected principal components. This projection creates a new dataset in a lower-dimensional space, where each data point is represented by its coordinates along these principal components."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f96a00f-ae47-4e37-bba3-631f73cc1b02",
   "metadata": {},
   "source": [
    "Here's a simple example to illustrate PCA:\n",
    "\n",
    "Suppose you have a dataset with two features, \"Height\" and \"Weight,\" and you want to reduce it to one dimension using PCA. The dataset looks like this:\n",
    "\n",
    "data = {'Height(in inches)':[65,72,68,74],'Weight(in pounds)':[140,175,160,180]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb0bf6cd-f8ce-4c8a-8e2e-bf0c50b4e777",
   "metadata": {},
   "source": [
    "1. Standardize the data (if necessary).\n",
    "2. Calculate the covariance matrix.\n",
    "3. Compute the eigenvalues and eigenvectors.\n",
    "4. Sort the eigenvalues in descending order.\n",
    "\n",
    "Let's say the sorted eigenvalues are λ1>λ2. You decide to keep the top principal component.\n",
    "1. Select the top eigenvector (principal component) corresponding to λ1. Let's call it PC1.\n",
    "2. Project the data onto PC1 to obtain the lower-dimensional representation:\n",
    "\n",
    "Projected_Data = [3.06,12.89,6.46,15.34]\n",
    "\n",
    "The original two features, \"Height\" and \"Weight,\" have been reduced to a single dimension, capturing most of the variance in the data. This reduction can be useful for visualization, noise reduction, or simplifying the dataset for further analysis while preserving the essential patterns and relationships in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce1adfa-3b58-45f4-bd0c-98d3893341f2",
   "metadata": {},
   "source": [
    "## Q4. What is the relationship between PCA and Feature Extraction, and how can PCA be used for Feature Extraction? Provide an example to illustrate this concept."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e924e3-fc5b-436c-8cf9-a1eb74844ad6",
   "metadata": {},
   "source": [
    "## Ans:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb14f63-903d-4b81-90e4-66d735664078",
   "metadata": {},
   "source": [
    "PCA (Principal Component Analysis) and feature extraction are closely related concepts, and PCA can be used as a technique for feature extraction. The primary goal of both PCA and feature extraction is to reduce the dimensionality of a dataset while retaining the most important information. Here's how PCA can be used for feature extraction:\n",
    "\n",
    "    Initial Dataset: Start with a high-dimensional dataset containing multiple features.\n",
    "\n",
    "    Standardization (Optional): You may choose to standardize the features to ensure that they have equal influence on the PCA process, especially if the features have different scales.\n",
    "\n",
    "    PCA: Apply PCA to the standardized dataset to identify the principal components (eigenvectors) and their corresponding eigenvalues.\n",
    "\n",
    "    Selection of Principal Components: Select a subset of the top principal components based on criteria such as explained variance or the number of dimensions you want to reduce to.\n",
    "\n",
    "    Projection: Project the original dataset onto the selected principal components to create a lower-dimensional representation of the data. This reduced representation effectively becomes a set of new features, which are linear combinations of the original features.\n",
    "\n",
    "    New Feature Space: These new features, formed by the projection, serve as a transformed representation of the data with reduced dimensionality. They are often referred to as the \"principal components\" or \"extracted features.\"\n",
    "    \n",
    "The relationship between PCA and feature extraction lies in the fact that PCA extracts linear combinations of the original features, known as principal components, that capture the most significant variance in the data. These principal components can be considered as new features that retain the essential information of the original data while reducing its dimensionality. In this way, PCA is a form of feature extraction.\n",
    "\n",
    "Let's illustrate this concept with an example:\n",
    "\n",
    "Suppose you have a dataset of images, and each image is represented as a matrix of pixel values. Each pixel can be considered a feature, making the dataset high-dimensional. You want to reduce the dimensionality of the dataset while preserving as much information as possible for tasks like image classification.\n",
    "\n",
    "    Initial Dataset: You have a dataset of grayscale images, where each image is 100x100 pixels, resulting in 10,000 features (pixels) per image.\n",
    "\n",
    "    Standardization (Optional): You may choose to standardize the pixel values to ensure that each pixel contributes equally to the PCA analysis.\n",
    "\n",
    "    PCA: Apply PCA to the dataset, resulting in a set of principal components ranked by their corresponding eigenvalues.\n",
    "\n",
    "    Selection of Principal Components: Based on your desired level of dimensionality reduction, you might decide to keep the top 50 principal components, for example.\n",
    "\n",
    "    Projection: Project the original images onto these 50 principal components to create a new representation for each image. These 50 values per image serve as the extracted features.\n",
    "\n",
    "Now, you have reduced the dimensionality of your image dataset from 10,000 pixels per image to just 50 extracted features, which capture the most significant patterns and variations in the images. These 50 features can be used as input for subsequent machine learning tasks, such as image classification or clustering, while significantly reducing computational complexity and potentially improving model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6beb0da0-b4c9-4260-a81b-9544a386dd73",
   "metadata": {},
   "source": [
    "## Q5. You are working on a project to build a recommendation system for a food delivery service. The dataset contains features such as price, rating, and delivery time. Explain how you would use Min-Max scaling to preprocess the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c41d0ce6-f3b1-4194-b343-d8ad36da7ad4",
   "metadata": {},
   "source": [
    "## Ans:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36afe1fd-1ae4-44ae-90ce-5cc84c2bad9a",
   "metadata": {},
   "source": [
    "To preprocess the dataset for building a recommendation system for a food delivery service, which includes features such as price, rating, and delivery time, you can use Min-Max scaling to ensure that all these features are on the same scale within a specified range (typically between 0 and 1). Min-Max scaling helps in standardizing the features and ensuring that they have a similar impact on the recommendation model, regardless of their original scales.\n",
    "\n",
    "Here's a step-by-step explanation of how you can use Min-Max scaling to preprocess the data:\n",
    "\n",
    "    Understand the Data: Start by understanding the dataset and the specific features you are working with. In your case, you mentioned that you have features like price, rating, and delivery time.\n",
    "\n",
    "    Identify the Range: Determine the desired range for Min-Max scaling. In most cases, you'd want to scale the features to a range between 0 and 1. However, you can choose a different range based on your project's requirements.\n",
    "\n",
    "    Find the Min and Max Values for Each Feature:Calculate the minimum (X_min) and maximum (X_max) values for each of the features you want to scale (price, rating, delivery time) within your dataset.\n",
    "\n",
    "    Apply Min-Max Scaling:For each feature, use the Min-Max scaling formula to scale the values. \n",
    "        \n",
    "    Repeat the Scaling for Each Feature: Apply the Min-Max scaling process independently to each of the features you want to scale (price, rating, and delivery time). This ensures that each feature is scaled based on its own minimum and maximum values.\n",
    "\n",
    "    Replace Original Values: Replace the original values in your dataset with the scaled values obtained from the Min-Max scaling process.\n",
    "\n",
    "After completing these steps, your dataset will have the features (price, rating, and delivery time) scaled to the specified range (e.g., [0, 1]). These scaled features can then be used as input for building your recommendation system, and the scaling ensures that no single feature dominates the recommendations due to its original scale. Min-Max scaling helps maintain the relative relationships between the features while bringing them to a common scale suitable for modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f26126a6-821b-46ba-8333-4617835da376",
   "metadata": {},
   "source": [
    "## Q6. You are working on a project to build a model to predict stock prices. The dataset contains many features, such as company financial data and market trends. Explain how you would use PCA to reduce the dimensionality of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc044cb6-b332-42f9-8217-8f0c34636822",
   "metadata": {},
   "source": [
    "## Ans:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e6e66bf-e521-4ad5-8295-b93a487e3844",
   "metadata": {},
   "source": [
    "Using Principal Component Analysis (PCA) to reduce the dimensionality of a dataset for predicting stock prices can be a valuable technique, especially when dealing with a large number of features. By reducing the dimensionality, you can simplify the dataset, remove noise, and potentially improve the performance of your stock price prediction model. Here's a step-by-step explanation of how you can use PCA for dimensionality reduction in this context:\n",
    "\n",
    "    Data Preprocessing:\n",
    "        Start by preprocessing your dataset, which includes features such as company financial data and market trends. This preprocessing may involve handling missing values, scaling the data, and ensuring that the features are in a suitable format.\n",
    "\n",
    "    Standardization (Optional):\n",
    "        Depending on the nature of your dataset, you may choose to standardize the features to ensure that they all have similar scales. Standardization is particularly important if the features have different units or scales, as PCA is sensitive to feature scales.\n",
    "\n",
    "    PCA Application:\n",
    "        Apply PCA to the preprocessed and standardized dataset. PCA will identify the principal components (PCs) and their corresponding eigenvalues.\n",
    "\n",
    "    Eigenvalue and Eigenvector Analysis:\n",
    "        Calculate the eigenvalues and eigenvectors associated with the covariance matrix of the dataset. Eigenvalues represent the variance explained by each principal component, and eigenvectors define the directions in feature space along which the data varies the most.\n",
    "\n",
    "    Sort Eigenvalues:\n",
    "        Sort the eigenvalues in descending order. This step helps you identify the most important principal components. You can decide how many components to keep based on the cumulative explained variance or by specifying the desired dimensionality reduction.\n",
    "\n",
    "    Select Principal Components:\n",
    "        Decide how many principal components (eigenvectors) to retain. You may choose to keep the top kk components, where kk is determined based on your desired level of dimensionality reduction. You can use techniques like explained variance or scree plots to make this decision.\n",
    "\n",
    "    Projection:\n",
    "        Project the original dataset onto the selected principal components. This projection creates a new dataset with reduced dimensionality, where each data point is represented by its coordinates along the chosen principal components.\n",
    "\n",
    "    Feature Engineering (Optional):\n",
    "        You can interpret the selected principal components as new features. These components are linear combinations of the original features and may represent underlying patterns in the data. Depending on the interpretability of these components, you can use them directly as features or combine them with other features for model building.\n",
    "\n",
    "    Model Building:\n",
    "        Use the reduced-dimension dataset, obtained after PCA, as input for building your stock price prediction model. Popular machine learning algorithms like regression, time series analysis, or deep learning can be applied to this dataset.\n",
    "\n",
    "Using PCA for dimensionality reduction can help you overcome the \"curse of dimensionality\" and improve model efficiency, especially when you have a large number of features. It can also aid in identifying the most important patterns and relationships in the data, potentially leading to better stock price predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1037b5c-813f-4af6-9caf-3ad5f1f9aed0",
   "metadata": {},
   "source": [
    "## Q7. For a dataset containing the following values: [1, 5, 10, 15, 20], perform Min-Max scaling to transform the values to a range of -1 to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bcb2550-ef8f-44f5-8776-968028282ca5",
   "metadata": {},
   "source": [
    "## Ans:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76da65b1-b6b2-4fb2-939d-271d490fa332",
   "metadata": {},
   "source": [
    "To perform Min-Max scaling on a dataset and transform the values to a range of -1 to 1, you'll need to follow these steps:\n",
    "\n",
    "    Find the minimum and maximum values in the dataset.\n",
    "    Apply the Min-Max scaling formula to each value in the dataset.\n",
    "    Scale the values to the desired range.\n",
    "\n",
    "Let's apply these steps to your dataset: [1, 5, 10, 15, 20].\n",
    "\n",
    "Step 1: Find the minimum and maximum values.\n",
    "\n",
    "    Minimum (X_min) = 1\n",
    "    Maximum (X_max) = 20\n",
    "\n",
    "Step 2: Apply the Min-Max scaling formula to each value.\n",
    "\n",
    "    For 1:\n",
    "    Xnorm=(1−1)/(20−1)=\n",
    "\n",
    "    For 5:\n",
    "    Xnorm=(5−1)/(20−1)=4/19≈0.2105\n",
    "\n",
    "    For 10:\n",
    "    Xnorm=(10−1)/(20−1)=9/19≈0.4737\n",
    "\n",
    "    For 15:\n",
    "    Xnorm=(15−1)/(20−1)=14/19≈0.7368\n",
    "\n",
    "    For 20:\n",
    "    Xnorm=(20−1)/(20−1)=1\n",
    "\n",
    "Step 3: Scale the values to the desired range of -1 to 1.\n",
    "\n",
    "    To transform the values from [0, 1] to [-1, 1], you can use the following transformation:\n",
    "    Xscaled=2⋅Xnorm−1\n",
    "\n",
    "Now, apply this transformation to each of the normalized values:\n",
    "\n",
    "    For 0:\n",
    "    Xscaled=2⋅0−1=−1\n",
    "\n",
    "    For 4/19≈0.2105:\n",
    "    Xscaled=2*0.2105−1≈−0.5789\n",
    "\n",
    "    For 9/19≈0.4737:\n",
    "    Xscaled=2*0.4737−1≈−0.0526\n",
    "\n",
    "    For 14/19≈0.7368:\n",
    "    Xscaled=2*0.7368−1≈0.4737\n",
    "\n",
    "    For 1:\n",
    "    Xscaled=2*1−1=1\n",
    "\n",
    "So, after Min-Max scaling, the values in the dataset [1, 5, 10, 15, 20] are transformed to the range of -1 to 1 as follows:[-1, -0.5789, -0.0526, 0.4737, 1]."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb9f78a-9484-4706-9aaf-edb1585202d1",
   "metadata": {},
   "source": [
    "## Q8. For a dataset containing the following features: [height, weight, age, gender, blood pressure], perform Feature Extraction using PCA. How many principal components would you choose to retain, and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cbd0785-16c0-4dfc-8678-05c0d0cfb372",
   "metadata": {},
   "source": [
    "## Ans:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe22c81-7cc4-431d-89b8-ce494190679f",
   "metadata": {},
   "source": [
    "Performing feature extraction using PCA involves reducing the dimensionality of a dataset by retaining a subset of its principal components. To decide how many principal components to retain, you typically consider the cumulative explained variance. The cumulative explained variance tells you how much of the total variance in the dataset is explained by the selected principal components. You aim to retain enough components to capture a significant portion of the variance while reducing dimensionality.\n",
    "\n",
    "Here are the steps to determine how many principal components to retain in your dataset with features [height, weight, age, gender, blood pressure]:\n",
    "\n",
    "    Data Preprocessing: Start by preprocessing your dataset, which includes handling missing values, standardizing the features if necessary, and ensuring that categorical variables like \"gender\" are appropriately encoded (e.g., one-hot encoding).\n",
    "\n",
    "    PCA Application: Apply PCA to the preprocessed dataset.\n",
    "\n",
    "    Eigenvalue and Eigenvector Calculation: Calculate the eigenvalues and eigenvectors associated with the covariance matrix of the dataset.\n",
    "\n",
    "    Sort Eigenvalues: Sort the eigenvalues in descending order.\n",
    "\n",
    "    Calculate Cumulative Explained Variance:\n",
    "        Calculate the cumulative explained variance for each number of retained principal components. You can use the following formula:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e56769-3a0b-4c89-b8ce-684fd7738cf3",
   "metadata": {},
   "source": [
    "Cumulative Explained Variance=\n",
    "$\\frac{\\sum_{i=1}^{k}\\lambda_{i}}{\\sum_{i=1}^{n}\\lambda_{i}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e528fdfa-e61b-4041-a5fe-b4ebe108fce0",
   "metadata": {},
   "source": [
    "Where:\n",
    "\n",
    "1. k is the number of principal components being considered.\n",
    "2. λi is the ii-th eigenvalue."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75792449-69cd-43d1-86f5-2e310134dea0",
   "metadata": {},
   "source": [
    "    Select Principal Components:\n",
    "        Examine the cumulative explained variance for different values of kk.\n",
    "        Choose a value of kk such that it captures a sufficiently high percentage of the total variance (e.g., 95% or 99%). This percentage depends on your specific application and the trade-off between dimensionality reduction and preserving information.\n",
    "\n",
    "    Project Data onto Selected Principal Components:\n",
    "        Once you've determined the number of principal components to retain, you can project the original data onto these components to obtain the reduced-dimensional representation.\n",
    "\n",
    "The decision of how many principal components to retain depends on your specific use case and the trade-off between dimensionality reduction and information preservation. A common practice is to choose a value of kk that captures a high percentage of the total variance while significantly reducing dimensionality. You might initially start with a conservative value of kk and then adjust it based on the cumulative explained variance analysis.\n",
    "\n",
    "In practice, you might find that, for some datasets, a small number of principal components can capture a substantial amount of variance, while in other cases, you may need to retain more components to achieve a similar level of information retention. Experimenting with different values of kk and evaluating the impact on your downstream tasks, such as predictive modeling, can help you make an informed decision on the number of principal components to retain."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
