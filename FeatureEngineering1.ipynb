{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b90c4c58-47d8-4c5c-9ef6-4499498432fc",
   "metadata": {},
   "source": [
    "## Q1: What are missing values in a dataset? Why is it essential to handle missing values? Name some algorithms that are not affected by missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "850a421c-a2e6-4ac0-877f-aa364e1079cf",
   "metadata": {},
   "source": [
    "## Ans:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f4bb5dc-189f-4210-81e8-ac743ee4ea79",
   "metadata": {},
   "source": [
    "Missing values in a dataset refer to the absence of data for one or more variables or observations. They can occur for various reasons, such as data collection errors, non-responses in surveys, or sensor malfunctions. Handling missing values is essential for several reasons:\n",
    "\n",
    "    Data Integrity: Missing values can introduce biases and inaccuracies into your analysis, leading to incorrect conclusions and predictions.\n",
    "\n",
    "    Reduced Sample Size: Ignoring missing values may result in a reduced sample size, potentially leading to less representative or less powerful analyses.\n",
    "\n",
    "    Bias in Results: Some statistical techniques may produce biased results or incorrect estimates if missing values are not properly handled.\n",
    "\n",
    "    Model Performance: Machine learning models can perform poorly when trained on data with missing values, as they may struggle to generalize from incomplete information.\n",
    "\n",
    "    Misleading Insights: Missing data can mislead analysts and decision-makers by distorting the true relationships within the data.\n",
    "\n",
    "To handle missing values, various techniques can be employed, including:\n",
    "\n",
    "    Imputation: This involves filling in missing values with estimated or substituted values. Common imputation methods include mean, median, mode imputation, or more advanced techniques like regression imputation.\n",
    "\n",
    "    Deletion: We can remove rows or columns with missing values. However, this should be done cautiously as it can lead to information loss and bias if not handled properly.\n",
    "\n",
    "    Prediction Models: We can use machine learning models to predict missing values based on the information available in the dataset.\n",
    "\n",
    "    Special Handling: For some cases, we might handle missing values differently, such as encoding missingness as a separate category or using domain-specific techniques.\n",
    "\n",
    "As for algorithms that are not affected by missing values, tree-based algorithms like Decision Trees, Random Forests, and Gradient Boosting Trees are relatively robust to missing data. They can handle missing values in a principled manner during the splitting process of the tree nodes. Additionally, k-Nearest Neighbors (KNN) is another algorithm that can work with missing values by finding similar data points for imputation. However, it's important to note that while these algorithms can handle missing data, the quality of imputation or the extent to which they perform well might still depend on the nature and extent of the missing data in our specific dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d72a7606-9dff-4281-9984-cdce5768325a",
   "metadata": {},
   "source": [
    "## Q2: List down techniques used to handle missing data. Give an example of each with python code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd9bfa41-206b-49ff-a471-787e48f01458",
   "metadata": {},
   "source": [
    "## Ans:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "705a157b-cdd1-4568-b04f-54b4b4027079",
   "metadata": {},
   "source": [
    "1. Mean/Median/Mode Imputation:\n",
    "This involves replacing missing values with the mean (for numerical data), median, or mode (for categorical data) of the respective feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa76535d-623c-4ab3-a797-04886280b22b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Age': [25, 30, None, 35, 40], 'Income': [50000, None, 60000, 70000, 80000]}\n",
      "    Age   Income\n",
      "0  25.0  50000.0\n",
      "1  30.0  50000.0\n",
      "2  32.5  60000.0\n",
      "3  35.0  70000.0\n",
      "4  40.0  80000.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Example DataFrame with missing values\n",
    "data = {'Age': [25, 30, None, 35, 40],\n",
    "        'Income': [50000, None, 60000, 70000, 80000]}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Mean imputation for numerical columns\n",
    "numerical_imputer = SimpleImputer(strategy='mean')\n",
    "df['Age'] = numerical_imputer.fit_transform(df[['Age']])\n",
    "\n",
    "# Mode imputation for categorical columns\n",
    "categorical_imputer = SimpleImputer(strategy='most_frequent')\n",
    "df['Income'] = categorical_imputer.fit_transform(df[['Income']])\n",
    "print(data)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf8caa0-0a32-4566-826f-8b34f18af783",
   "metadata": {},
   "source": [
    "2. Forward Fill and Backward Fill:\n",
    "In time series data, we can use forward fill (propagating the last known value) or backward fill (propagating the next known value) to fill missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ca2a13c-9668-4187-9501-3bb0e208a9f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Date': DatetimeIndex(['2023-01-01', '2023-01-02', '2023-01-03', '2023-01-04',\n",
      "               '2023-01-05'],\n",
      "              dtype='datetime64[ns]', freq='D'), 'Stock_Price': [100, None, None, 110, None]}\n",
      "        Date  Stock_Price\n",
      "0 2023-01-01        100.0\n",
      "1 2023-01-02        100.0\n",
      "2 2023-01-03        100.0\n",
      "3 2023-01-04        110.0\n",
      "4 2023-01-05        110.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Example DataFrame with missing values in a time series\n",
    "data = {'Date': pd.date_range(start='2023-01-01', periods=5, freq='D'),\n",
    "        'Stock_Price': [100, None, None, 110, None]}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Forward fill to propagate previous value\n",
    "df['Stock_Price'].fillna(method='ffill', inplace=True)\n",
    "\n",
    "# Backward fill to propagate next value\n",
    "df['Stock_Price'].fillna(method='bfill', inplace=True)\n",
    "print(data)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c186fdde-746c-4157-9796-22860a6577b7",
   "metadata": {},
   "source": [
    "3. K-Nearest Neighbors (KNN) Imputation:\n",
    "KNN imputation fills missing values by finding k-nearest neighbors based on the available features and using their values to impute the missing ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "59fa599d-fab4-4625-b4a2-0313566fcda4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Feature1': [1, 2, None, 4, 5], 'Feature2': [2, None, 4, 6, None]}\n",
      "   Feature1  Feature2\n",
      "0       1.0       2.0\n",
      "1       2.0       4.0\n",
      "2       2.5       4.0\n",
      "3       4.0       6.0\n",
      "4       5.0       4.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "# Example DataFrame with missing values\n",
    "data = {'Feature1': [1, 2, None, 4, 5],\n",
    "        'Feature2': [2, None, 4, 6, None]}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# KNN imputation\n",
    "imputer = KNNImputer(n_neighbors=2)\n",
    "df_imputed = imputer.fit_transform(df)\n",
    "df_imputed = pd.DataFrame(df_imputed, columns=df.columns)\n",
    "print(data)\n",
    "print(df_imputed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d891b93-90f2-4638-996a-bdc6a13f1a68",
   "metadata": {},
   "source": [
    "4. Deletion of Rows or Columns:\n",
    "We can remove rows or columns with missing values using the dropna() method in pandas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7dd6a410-f470-416c-9227-84fbd1e8aefd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Example DataFrame with missing values\n",
    "data = {'A': [1, None, 3, 4, 5],\n",
    "        'B': [None, 2, None, None, 6]}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Remove rows with missing values\n",
    "df_cleaned_rows = df.dropna()\n",
    "\n",
    "# Remove columns with missing values\n",
    "df_cleaned_cols = df.dropna(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b0a4547-e593-4a63-b50d-47d3dc1dd7ec",
   "metadata": {},
   "source": [
    "## Q3: Explain the imbalanced data. What will happen if imbalanced data is not handled?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb89789-668e-4f8d-8f45-4639f15040ab",
   "metadata": {},
   "source": [
    "## Ans:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2708b230-e24d-4d26-b835-e8dae30c9c60",
   "metadata": {},
   "source": [
    "Imbalanced data refers to a situation in a classification problem where the classes are not represented equally or nearly equally in the dataset. In other words, one class (the minority class) has significantly fewer instances than the other class or classes (the majority class or classes). Imbalanced data is a common issue in many real-world machine learning applications, including fraud detection, medical diagnosis, and text classification.\n",
    "\n",
    "Here's what can happen if imbalanced data is not handled properly:\n",
    "\n",
    "    Biased Model: Machine learning algorithms tend to be biased toward the majority class because they aim to maximize overall accuracy. As a result, the model may not perform well in predicting the minority class. In many cases, the model might simply predict the majority class for all instances.\n",
    "\n",
    "    Misleading Evaluation Metrics: When we evaluate the performance of a model on imbalanced data using traditional metrics like accuracy, it can be deceptive. A model that predicts the majority class most of the time can still achieve high accuracy, but it may fail to capture the minority class, which is often the more critical class to detect.\n",
    "\n",
    "    Loss of Information: Ignoring the minority class can lead to a loss of important information. For instance, in a medical diagnosis scenario, failing to detect rare diseases can have serious consequences.\n",
    "\n",
    "    Model Overfitting: Imbalanced data can lead to overfitting, where the model fits noise in the majority class rather than learning meaningful patterns. This can result in poor generalization to new data.\n",
    "\n",
    "To address imbalanced data, several techniques can be employed:\n",
    "\n",
    "    Resampling: This involves either oversampling the minority class (creating more instances of the minority class) or undersampling the majority class (removing some instances of the majority class) to balance the class distribution.\n",
    "\n",
    "    Synthetic Data Generation: Techniques like Synthetic Minority Over-sampling Technique (SMOTE) create synthetic samples for the minority class to balance the dataset.\n",
    "\n",
    "    Cost-sensitive Learning: Assigning different misclassification costs to different classes can encourage the model to pay more attention to the minority class.\n",
    "\n",
    "    Ensemble Methods: Algorithms like Random Forest and Gradient Boosting can be modified to give more weight to the minority class during training.\n",
    "\n",
    "    Anomaly Detection: Treat the problem as an anomaly detection task where the minority class is treated as the anomaly to be detected.\n",
    "\n",
    "    Different Evaluation Metrics: Use evaluation metrics like precision, recall, F1-score, area under the ROC curve (AUC-ROC), and area under the precision-recall curve (AUC-PR) instead of accuracy to assess model performance on imbalanced data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce5ceea-551b-4cc3-85fd-f94407b478ce",
   "metadata": {},
   "source": [
    "## Q4: What are Up-sampling and Down-sampling? Explain with an example when up-sampling and down-sampling are required."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dffea266-e2a3-492a-9c29-cf25f65bc307",
   "metadata": {},
   "source": [
    "## Ans:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "674727aa-7a14-4708-ab76-061d1b25558d",
   "metadata": {},
   "source": [
    "Up-Sampling:\n",
    "\n",
    "    Definition: Up-sampling involves increasing the number of instances in the minority class (the class with fewer examples) by generating additional samples.\n",
    "    When to Use Up-Sampling:\n",
    "        Up-sampling is required when the minority class is underrepresented, and we want to balance the class distribution.\n",
    "        It can be useful when we have limited data for the minority class, and creating synthetic samples is a viable option.\n",
    "    Example:\n",
    "    Suppose we're working on a credit card fraud detection task, where fraud cases are rare (minority class) compared to non-fraudulent transactions (majority class). You can up-sample the fraud cases by generating synthetic samples to balance the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2529f97c-0e47-44dd-9df8-bc0ae2a390ab",
   "metadata": {},
   "source": [
    "Down-Sampling:\n",
    "\n",
    "    Definition: Down-sampling involves reducing the number of instances in the majority class (the class with more examples) to match the number of instances in the minority class.\n",
    "    When to Use Down-Sampling:\n",
    "        Down-sampling is necessary when we have an excessive number of examples in the majority class, leading to class imbalance.\n",
    "        It can be used when we have sufficient data for the majority class, and removing some instances does not significantly impact the overall information content.\n",
    "    Example:\n",
    "    Consider a medical diagnosis task where we're predicting a rare disease (minority class) in a large population. If the dataset contains a disproportionately large number of healthy individuals (majority class), we can down-sample the healthy cases to balance the dataset and focus the model's attention on the disease detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "327bbb41-3bb0-4cef-ad6c-7948bfa443f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "# Up-sampling: Randomly generates synthetic samples for the minority class\n",
    "up_sampler = RandomOverSampler(sampling_strategy='minority')\n",
    "X_upsampled, y_upsampled = up_sampler.fit_resample(X, y)\n",
    "\n",
    "# Down-sampling: Reduces the number of instances in the majority class\n",
    "down_sampler = RandomUnderSampler(sampling_strategy='majority')\n",
    "X_downsampled, y_downsampled = down_sampler.fit_resample(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3990e5-7c3f-418b-8320-f8ec4bbb26f2",
   "metadata": {},
   "source": [
    "## Q5: What is data Augmentation? Explain SMOTE."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6feca753-c6e6-4da3-a0cb-73ad34a0e048",
   "metadata": {},
   "source": [
    "## Ans:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d2ad55-fedd-48ac-ae14-2a2313120712",
   "metadata": {},
   "source": [
    "Data augmentation is a technique commonly used in machine learning, especially for tasks like computer vision and natural language processing, where the availability of labeled data can be limited. Data augmentation aims to increase the size and diversity of a dataset by applying various transformations or perturbations to the existing data, creating new samples that are similar to the original ones but exhibit some variability. This technique helps improve the generalization and robustness of machine learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c395699-086d-4fda-acfd-80528a5bf9a9",
   "metadata": {},
   "source": [
    "SMOTE (Synthetic Minority Over-sampling Technique) is a specific data augmentation technique used to address class imbalance in classification problems, particularly when dealing with imbalanced datasets. SMOTE focuses on the minority class and generates synthetic samples for it by interpolating between existing minority class instances. Here's how SMOTE works:\n",
    "\n",
    "    Selecting a Minority Instance: For each minority class instance (sample), SMOTE selects one or more of its nearest neighbors in the feature space. The number of neighbors to select is a user-defined parameter.\n",
    "\n",
    "    Creating Synthetic Samples: SMOTE generates synthetic samples by linearly interpolating between the selected instance and its chosen neighbors. This involves creating new data points along the line segments connecting the instance to its neighbors.\n",
    "\n",
    "    Balancing the Dataset: The synthetic samples are added to the original dataset, effectively balancing the class distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594034fd-8169-4688-8a99-32bff821b080",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Create an instance of the SMOTE class\n",
    "smote = SMOTE(sampling_strategy='auto', random_state=42)\n",
    "\n",
    "# Apply SMOTE to the dataset\n",
    "X_resampled, y_resampled = smote.fit_resample(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a677d2ad-63b2-4f30-a64a-11a237caf22a",
   "metadata": {},
   "source": [
    "## Q6: What are outliers in a dataset? Why is it essential to handle outliers?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "857c237e-4586-4835-b614-fc601ae429c4",
   "metadata": {},
   "source": [
    "## Ans:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb7a85cc-26c1-48bf-8526-9d27438e3fbf",
   "metadata": {},
   "source": [
    "Outliers in a dataset are data points that significantly differ from the rest of the data, often due to errors in data collection, measurement noise, or rare events. Outliers can be unusually high or low values and can have a substantial impact on statistical analysis and machine learning models. Here's why handling outliers is essential:\n",
    "\n",
    "    Influence on Descriptive Statistics: Outliers can significantly skew summary statistics such as the mean and standard deviation. The mean, in particular, can be strongly influenced by extreme values, leading to a misrepresentation of the central tendency of the data.\n",
    "\n",
    "    Impact on Data Visualization: Outliers can distort data visualizations, making it challenging to interpret and draw meaningful insights from charts and graphs.\n",
    "\n",
    "    Inaccurate Model Training: Machine learning algorithms can be sensitive to outliers. Outliers may affect the coefficients of models like linear regression or the split points in decision trees, resulting in models that do not generalize well to new data.\n",
    "\n",
    "    Decreased Model Performance: Outliers can lead to decreased model performance, particularly in algorithms like k-nearest neighbors (KNN) and clustering, where distances between data points play a crucial role.\n",
    "\n",
    "    Violation of Assumptions: Many statistical techniques and machine learning algorithms assume that the data is normally distributed or free from extreme values. Outliers can violate these assumptions, leading to incorrect or biased results.\n",
    "\n",
    "To handle outliers, several techniques can be applied:\n",
    "\n",
    "    Visual Inspection: Start by visualizing our data using box plots, scatter plots, histograms, or other relevant visualizations to identify potential outliers.\n",
    "\n",
    "    Statistical Methods: Use statistical methods like the Z-score or modified Z-score to detect outliers based on how many standard deviations they are from the mean.\n",
    "\n",
    "    Interquartile Range (IQR): Calculate the IQR (the difference between the 75th and 25th percentiles) and identify data points that fall outside a certain range of the IQR. This method is robust to outliers.\n",
    "\n",
    "    Data Transformation: Apply data transformations such as logarithmic or square root transformations to reduce the impact of extreme values.\n",
    "\n",
    "    Winsorization: Winsorization involves capping extreme values by replacing them with a specified percentile value (e.g., 99th percentile).\n",
    "\n",
    "    Removing or Truncating: In some cases, we may choose to remove or truncate extreme values if they are due to data entry errors or have no meaningful interpretation in the context of your analysis.\n",
    "\n",
    "    Robust Algorithms: Use machine learning algorithms that are less sensitive to outliers, such as robust regression methods or tree-based models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cbeb638-3227-4c77-928b-4afa2ed1d0e4",
   "metadata": {},
   "source": [
    "## Q7: You are working on a project that requires analyzing customer data. However, you notice that some of the data is missing. What are some techniques you can use to handle the missing data in your analysis?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4973f917-b3f9-4dbd-8cce-709e1d0f533a",
   "metadata": {},
   "source": [
    "## Ans:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd35b3da-7b61-4ce1-99c8-88fb7120012e",
   "metadata": {},
   "source": [
    "Data Imputation:\n",
    "\n",
    "    Mean, Median, or Mode Imputation: Replace missing values in numerical features with the mean, median, or mode of that feature.\n",
    "    Regression Imputation: Use regression models to predict missing values based on other relevant features.\n",
    "    K-Nearest Neighbors (KNN) Imputation: Impute missing values by averaging or voting among the K-nearest neighbors of the data point with missing values.\n",
    "\n",
    "Deletion:\n",
    "\n",
    "    Listwise Deletion: Remove entire rows with missing data. Use this cautiously as it can result in a loss of valuable information and potentially biased analyses if missingness is not random.\n",
    "    Column Deletion: Remove entire columns (features) with a high proportion of missing values if those features are not critical for our analysis.\n",
    "\n",
    "Forward Fill and Backward Fill:\n",
    "\n",
    "    For time series data, use forward fill (propagate the last known value) or backward fill (propagate the next known value) to fill missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ffa52ac-5943-4b78-a05c-9fdfca9f0ee8",
   "metadata": {},
   "source": [
    "## Q8: You are working with a large dataset and find that a small percentage of the data is missing. What are some strategies you can use to determine if the missing data is missing at random or if there is a pattern to the missing data?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3510e5a1-6846-4b2b-ab94-765015b87786",
   "metadata": {},
   "source": [
    "## Ans:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18faeaf2-38d3-4023-8f00-a4b3d7784490",
   "metadata": {},
   "source": [
    "When dealing with a large dataset with missing data, it's important to determine whether the missing data is missing completely at random (MCAR), missing at random (MAR), or missing not at random (MNAR). Here are some strategies and techniques to help you assess the pattern of missing data:\n",
    "\n",
    "    Data Visualization:\n",
    "        Create visualizations, such as histograms, bar charts, or heatmaps, to visualize missing values. Visual patterns may provide insights into the missing data mechanism.\n",
    "\n",
    "    Summary Statistics:\n",
    "        Calculate summary statistics (e.g., means, medians, standard deviations) for both the complete and missing data subsets. Compare these statistics to see if there are noticeable differences.\n",
    "\n",
    "    Missing Data Heatmap:\n",
    "        Create a heatmap that shows the correlation between missing values in different variables. If missing values tend to occur together in specific variables, it may indicate a pattern.\n",
    "\n",
    "    Missing Data Indicator Variable:\n",
    "        Create a binary indicator variable for each feature that indicates whether the data is missing or not. Then, calculate correlations between these indicators and other variables to identify any relationships.\n",
    "\n",
    "    Pattern Analysis:\n",
    "        Examine the missing data patterns across different subsets of the data, such as by time periods, geographic regions, or demographic groups. If patterns emerge within these subsets, it may suggest non-random missingness.\n",
    "\n",
    "    Statistical Tests:\n",
    "        Perform statistical tests to assess the relationship between missingness and other variables. For example, you can use chi-squared tests for categorical data or t-tests for continuous data to test whether the missingness is related to certain factors.\n",
    "\n",
    "    Machine Learning Models:\n",
    "        Train machine learning models to predict missing values based on other features. If the models perform well, it suggests that the missing data may be predictable and not entirely random.\n",
    "\n",
    "    Consult Domain Experts:\n",
    "        Consult with domain experts or individuals who have a deep understanding of the data to gather insights into potential reasons for missing data. They may provide valuable context about why certain data points are missing.\n",
    "\n",
    "    Explore Data Collection Process:\n",
    "        Investigate the data collection process and data entry procedures to identify any systematic errors or issues that could lead to non-random missingness.\n",
    "\n",
    "    Missing Data Mechanism Tests:\n",
    "        Use formal tests to assess the missing data mechanism, such as Little's MCAR test or other diagnostic tests designed to determine whether the data is missing completely at random."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e236568-cab5-40e2-9b4b-95b2a50d4a96",
   "metadata": {},
   "source": [
    "## Q9: Suppose you are working on a medical diagnosis project and find that the majority of patients in the dataset do not have the condition of interest, while a small percentage do. What are some strategies you can use to evaluate the performance of your machine learning model on this imbalanced dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3b4bd2-46b2-4d99-86e3-af5adad3dd4a",
   "metadata": {},
   "source": [
    "## Ans:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "455047ae-8804-4ae3-b7d2-b92583705f59",
   "metadata": {},
   "source": [
    "When dealing with an imbalanced dataset, such as in a medical diagnosis project where the majority of patients do not have the condition of interest, it's essential to choose evaluation strategies that account for the class imbalance. Here are some strategies you can use to evaluate the performance of your machine learning model in this scenario:\n",
    "\n",
    "    Confusion Matrix and Class Metrics:\n",
    "        Use a confusion matrix to break down the model's predictions into true positives, true negatives, false positives, and false negatives.\n",
    "        Calculate class-specific metrics, such as precision, recall (sensitivity), specificity, and F1-score for both the minority (positive) and majority (negative) classes.\n",
    "\n",
    "    Receiver Operating Characteristic (ROC) Curve and Area Under the Curve (AUC-ROC):\n",
    "        Plot an ROC curve to visualize the trade-off between true positive rate (TPR) and false positive rate (FPR) at different classification thresholds.\n",
    "        Calculate the AUC-ROC score, which quantifies the model's ability to discriminate between the two classes. A higher AUC-ROC indicates better performance.\n",
    "\n",
    "    Precision-Recall Curve and Area Under the Curve (AUC-PR):\n",
    "        Plot a precision-recall curve to assess the precision-recall trade-off at different thresholds.\n",
    "        Calculate the AUC-PR score, which measures the model's performance concerning precision and recall, especially important for imbalanced datasets.\n",
    "\n",
    "    Balanced Accuracy:\n",
    "        Calculate balanced accuracy, which considers the average of sensitivity and specificity and is less affected by class imbalance.\n",
    "\n",
    "    Class Weighting:\n",
    "        Assign different misclassification costs (class weights) to the minority and majority classes during model training. This encourages the model to pay more attention to the minority class.\n",
    "\n",
    "    Resampling Techniques:\n",
    "        Implement resampling methods like oversampling the minority class or undersampling the majority class to balance the dataset before training the model. Evaluate the model on the resampled dataset.\n",
    "\n",
    "    Cost-sensitive Learning:\n",
    "        Utilize cost-sensitive learning techniques that assign different misclassification costs to different classes, emphasizing the importance of correct classification for the minority class.\n",
    "\n",
    "    Ensemble Models:\n",
    "        Train ensemble models like Random Forest or Gradient Boosting, which can handle class imbalance more effectively by combining multiple base models.\n",
    "\n",
    "    Threshold Adjustment:\n",
    "        Adjust the classification threshold to balance precision and recall based on the specific requirements of the application. This can be especially useful in situations where one metric is more critical than the other.\n",
    "\n",
    "    Cross-Validation:\n",
    "        Use techniques like stratified k-fold cross-validation to ensure that each fold maintains the class distribution, providing a more robust estimate of model performance.\n",
    "\n",
    "    Anomaly Detection:\n",
    "        Consider treating the problem as an anomaly detection task, with the minority class as the anomaly to be detected.\n",
    "\n",
    "    Domain Expertise:\n",
    "        Consult with domain experts to determine the appropriate trade-offs between precision and recall and establish realistic performance expectations for the application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930cea25-20cc-4cb1-954b-b75287a8e995",
   "metadata": {},
   "source": [
    "## Q10: When attempting to estimate customer satisfaction for a project, you discover that the dataset is unbalanced, with the bulk of customers reporting being satisfied. What methods can you employ to balance the dataset and down-sample the majority class?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db25a8c8-3811-4096-b1a9-8174ec4a5251",
   "metadata": {},
   "source": [
    "## Ans:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f861a32-4a41-4d7e-b9df-fd7ed04394db",
   "metadata": {},
   "source": [
    "When dealing with an imbalanced dataset in a customer satisfaction project where the majority of customers report being satisfied, we can employ various methods to balance the dataset by down-sampling the majority class. Down-sampling involves reducing the number of instances in the majority class to match the number of instances in the minority class. Here are some techniques we can use:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d53bb5-271a-442d-bc01-522128c5932e",
   "metadata": {},
   "source": [
    "Random Under-sampling:\n",
    "    \n",
    "    Randomly select a subset of data points from the majority class to match the size of the minority class. This can be a straightforward and effective method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca74941-4dcd-4e7d-868b-97b6a385c7a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n",
    "\n",
    "# Down-sample the majority class\n",
    "df_majority = df[df['satisfaction'] == 'satisfied']\n",
    "df_minority = df[df['satisfaction'] == 'not_satisfied']\n",
    "\n",
    "df_majority_downsampled = resample(df_majority, replace=False, n_samples=len(df_minority), random_state=42)\n",
    "df_downsampled = pd.concat([df_majority_downsampled, df_minority])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "394e861d-7d14-4ac1-82b2-e70331b0c299",
   "metadata": {},
   "source": [
    "Cluster-based Under-sampling:\n",
    "\n",
    "    Use clustering algorithms like k-means to group similar data points in the majority class, and then randomly select one representative data point from each cluster.\n",
    "\n",
    "Tomek Links:\n",
    "\n",
    "    Identify pairs of instances (one from the majority class and one from the minority class) that are nearest neighbors but of different classes. Remove the majority class instance in each pair.\n",
    "\n",
    "Edited Nearest Neighbors (ENN):\n",
    "\n",
    "    Remove data points from the majority class that are misclassified by their k-nearest neighbors.\n",
    "\n",
    "NearMiss Algorithm:\n",
    "\n",
    "    Use the NearMiss algorithm, which selects data points from the majority class that are closest to the minority class instances.\n",
    "\n",
    "SMOTE with Tomek Links (SMOTE-TL):\n",
    "\n",
    "    Apply Synthetic Minority Over-sampling Technique (SMOTE) to generate synthetic minority class samples and then remove the samples that form Tomek links with the majority class.\n",
    "\n",
    "Combining Techniques:\n",
    "\n",
    "    You can combine multiple down-sampling techniques to further balance the dataset. For example, you can apply random under-sampling after using SMOTE to generate synthetic minority samples.\n",
    "\n",
    "Stratified Sampling:\n",
    "\n",
    "    If the dataset is exceptionally large, you can perform stratified sampling by randomly selecting a subset of the majority class instances while maintaining the overall class proportions.\n",
    "\n",
    "Custom Down-sampling Strategies:\n",
    "\n",
    "    Depending on the specific characteristics of your dataset, you may develop custom down-sampling strategies that consider domain knowledge or specific project requirements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61dd535c-1ab8-4278-85bf-bb93d060cf34",
   "metadata": {},
   "source": [
    "## Q11: You discover that the dataset is unbalanced with a low percentage of occurrences while working on a project that requires you to estimate the occurrence of a rare event. What methods can you employ to balance the dataset and up-sample the minority class?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d95ecd-5cf2-4c2d-80cc-e6b09c3461d6",
   "metadata": {},
   "source": [
    "## Ans:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f4c530-4eca-4b8d-915b-0aabd2c392f1",
   "metadata": {},
   "source": [
    "When dealing with an imbalanced dataset where the occurrence of a rare event is underrepresented, you can employ various methods to balance the dataset by up-sampling the minority class. Up-sampling involves increasing the number of instances in the minority class to match the size of the majority class. Here are some techniques you can use:\n",
    "\n",
    "    Random Over-sampling:\n",
    "        Randomly duplicate data points from the minority class to match the size of the majority class. This is a simple and effective method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e7a078-577f-429b-a235-537ee0115ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n",
    "\n",
    "# Up-sample the minority class\n",
    "df_majority = df[df['occurrence'] == 'not_rare']\n",
    "df_minority = df[df['occurrence'] == 'rare']\n",
    "\n",
    "df_minority_upsampled = resample(df_minority, replace=True, n_samples=len(df_majority), random_state=42)\n",
    "df_upsampled = pd.concat([df_majority, df_minority_upsampled])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef8a3f1a-56fa-42b3-9a9f-46cc503bfd62",
   "metadata": {},
   "source": [
    "SMOTE (Synthetic Minority Over-sampling Technique):\n",
    "\n",
    "    Generate synthetic samples for the minority class by interpolating between existing minority class instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609c8221-e8c3-41b1-838d-6f5768611a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "smote = SMOTE(sampling_strategy='auto', random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c9d246-bc66-46a1-b514-3299433fc18e",
   "metadata": {},
   "source": [
    "ADASYN (Adaptive Synthetic Sampling):\n",
    "\n",
    "    Similar to SMOTE but focuses on generating synthetic samples for the minority class near the decision boundary to improve the model's ability to discriminate between classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee1a8e6-54d9-445a-911e-55b44ca9156f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import ADASYN\n",
    "\n",
    "adasyn = ADASYN(sampling_strategy='auto', random_state=42)\n",
    "X_resampled, y_resampled = adasyn.fit_resample(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "372533cb-5a20-4591-9914-a5f023ae9e26",
   "metadata": {},
   "source": [
    "SMOTE-ENN (SMOTE with Edited Nearest Neighbors):\n",
    "\n",
    "    Combine SMOTE with editing by removing noisy or borderline samples using Edited Nearest Neighbors.\n",
    "\n",
    "SMOTE-Tomek (SMOTE with Tomek Links):\n",
    "\n",
    "    Combine SMOTE with Tomek Links to remove synthetic samples that form Tomek links with the majority class.\n",
    "\n",
    "Cluster-based Over-sampling:\n",
    "\n",
    "    Use clustering algorithms to identify clusters within the minority class and generate synthetic samples based on the cluster centers.\n",
    "\n",
    "Bootstrapping:\n",
    "\n",
    "    Randomly sample the minority class with replacement to create additional instances. This method is similar to random over-sampling but allows for duplicates.\n",
    "\n",
    "Generative Adversarial Networks (GANs):\n",
    "\n",
    "    Train a GAN to generate realistic samples of the minority class.\n",
    "\n",
    "Ensemble Techniques:\n",
    "\n",
    "    Use ensemble methods like EasyEnsemble or BalanceCascade, which combine multiple models trained on different subsets of the minority class.\n",
    "\n",
    "Cost-sensitive Learning:\n",
    "\n",
    "    Assign different misclassification costs to different classes during model training, emphasizing the importance of the minority class."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
